1. clip gradient: 有效的防止梯度爆炸，loss divergence，防止某次迭代梯度更新过猛
2. 当label语义混合时，可以采用如下策略：预测到混合label中的一个，将top5中最后一个踢掉，换成语义相近的label。
3. bottleneck：用于控制和过滤特征数，认为设定的特征门限，一般来说要小于前层的特征数量，略大于后层的数量或类别数。
4. 优化器选择：
   1）Batch gradient descent： 采用整个训练数据计算梯度，缺点在于由于这种方法是在一次更新中，就对整个数据集计算梯度，所以计算起来非常慢，遇到很大量的数据集也会非常棘手，而且不能投入新数据实时更新模型。优点在于Batch gradient descent 对于凸函数可以收敛到全局极小值，对于非凸函数可以收敛到局部极小值。
   2）Stochastic gradient descent：和 BGD 的一次用所有数据计算梯度相比，SGD 每次更新时对每个样本进行梯度更新， 对于很大的数据集来说，可能会有相似的样本，这样 BGD 在计算梯度时会出现冗余， 而 SGD 一次只进行一次更新，就没有冗余，而且比较快，并且可以新增样本。缺点是SGD 因为更新比较频繁，会造成 cost function 有严重的震荡。与BGD相比，BGD 可以收敛到局部极小值，当然 SGD 的震荡可能会跳到更好的局部极小值处。当我们稍微减小 learning rate，SGD 和 BGD 的收敛性是一样的。
   3）Mini-batch gradient descent：MBGD 每一次利用一小批样本，即 n 个样本进行计算，这样它可以降低参数更新时的方差，收敛更稳定，另一方面可以充分地利用深度学习库中高度优化的矩阵操作来进行更有效的梯度计算。缺点在于不能保证很好的收敛性：
     1. learning rate 如果选择的太小，收敛速度会很慢，如果太大，loss function 就会在极小值处不停地震荡甚至偏离。
      （有一种措施是先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小 learning rate，不过这个阈值的设定需要提前写好，这样的话就不能够适应数据集的特点）
     2. 此外，这种方法是对所有参数更新时应用同样的 learning rate，如果我们的数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新。
     3. 另外，对于非凸函数，还要避免陷于局部极小值处，或者鞍点处，因为鞍点周围的error 是一样的，所有维度的梯度都接近于0，SGD 很容易被困在这里。
     4. 选择合适的learning rate比较困难，对所有的参数更新使用同样的learning rate。对于稀疏数据或者特征，有时我们可能想更新快一些对于不经常出现的特征，对于常出现的特征更新慢一些，这时候SGD就不太能满足要求了。
     为了解决上述问题，出现了下面的算法：
  4）Momentum：SGD 在 ravines 的情况下容易被困住， ravines 就是曲面的一个方向比另一个方向更陡，这时 SGD 会发生震荡而迟迟不能接近极小值，Momentum相当于引入了物理学中动量，这种情况相当于小球从山上滚下来时是在盲目地沿着坡滚，如果它能具备一些先知，例如快要上坡时，就知道需要减速了的话，适应性会更好。动量一般设为0.9.
  5）Nesterov accelerated gradient：NAG是对momentum的改进，对循环神经网络来说效果更好。
  6）Adagrad：这个算法就可以对低频的参数做较大的更新，对高频的做较小的更新，也因此，对于稀疏的数据它的表现很好，很好地提高了 SGD 的鲁棒性，前期约束较大，会对梯度进行约束，后期约束较小会对梯度进行放大，例如识别 Youtube 视频里面的猫，训练 GloVe word embeddings，因为它们都是需要在低频的特征上有更大的更新。它的缺点是分母会不断积累，这样学习率就会收缩并最终会变得非常小，会使梯度趋近于0。因此会使收敛变慢，甚至于提前结束。
  7）Adadelta：对Adagrad的改进，无需再事先设置学习率。训练前期收敛效果不错，加速较快，后期反复在局部最小值附近抖动。
  8）RMSprop：Hittton提出的，可以在plateau的时候自己进行调整学习率。
  9）Adam：另一种计算每个参数的自适应学习率的方法。对内存需求较小，为不同的参数计算不同的自适应学习率，也适用于大多非凸优化，适用于大数据集和高维空间。
  总结：
  如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam。RMSprop, Adadelta, Adam 在很多情况下的效果是相似的。Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，随着梯度变的稀疏，Adam 比 RMSprop 效果会好。整体来讲，Adam 是最好的选择。
5. 3D对时序把握更好，但是2D可以关注更多的空间信息，针对场景大的数据来说可能效果更好。
6. GRU是LSTM的变种，将LSTM的输入门、输出门、遗忘门改为更新门与重置门。
7. 网络设计原则：c*h*w经过多层layer仍旧保持一致。
8. caffe pytorch caffe2中dropout概率是置零的概率，而tensorflow是保留的概率。
9. 写动态规划算法时遇到数组，命名可以选用dp.
