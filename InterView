1. 介绍项目，推导MV1与MV2，tracking模块，检测模块的改进，在此基础上看对方是否了解行为识别，决定石油项目怎么讲。
   mv1参数量减少的推导，depth的作用是将spatial与channel解耦，且加了depth_multiplier控制通道数；MV2结构划分：Linear bottleneck + inversted residual block，resnet block为啥是先降维再升维？降低参数，并保持特征维度一致，为了shortcut。而mv2中先升维后降维是为了信息更好的传递，然后接线性层，之所以为了接线性层是因为如果用relu之类的激活函数会造成信息丢失的比较厉害（维度较小的情况下）。
2. L0、L1 与 L2的区别，为什么L1 会造成稀疏权重，L2造成趋近于0的权重,加了L1、L2为什么还会收敛
   答：L0是非零元素的个数，实现特征的自动选择，去除无用特征。稀疏化可以去掉这些无用特征，将特征对应的权重置为零。L1是各元素的绝对值之和，L1范数也可以实现稀疏，通过将无用特征对应的参数W置为零实现。L0和L1都可以实现稀疏化，不过一般选用L1而不用L0，原因包括：1）L0范数很难优化求解（NP难）；2）L1是L0的最优凸近似，比L0更容易优化求解。L2是各元素平方和再开方，L2避免过拟合的原理是：让L2范数的规则项||W||2 尽可能小，可以使得W每个元素都很小，接近于零，但是与L1不同的是，不会等于0；这样得到的模型抗干扰能力强，参数很小时，即使样本数据x发生很大的变化，模型预测值y的变化也会很有限。L1造成稀疏可以用参数更新来解释，在迭代几次之和容易求得0值，L2则是容易靠近到0，而不是直接减为0.加上他们的好处在于参数稀疏化，可以简化模型，避免过拟合，因为真正可用的参数并不多。使得参数值变小，参数越小模型越简单。
3. LR、SVM推导、核函数，什么时候用核函数，SVM输入怎么选择，LR和SVM怎么选择
   当特征在低维不可分时，需要利用f(x)将x映射到高维空间，但是会出现内积f(i)f(j)的计算，其计算维度会非常的大，因此利用核函数进行计算。常见核函数的展开
及其参数调整，有两个重要的参数即C、Gamma，前者为惩罚项，越大容易过拟合，越小容易欠拟合，后者越大支持向量越少，越小支持向量越大。至于特征选择不是很熟悉，了解一下鸟窝的处理，SVM好像要把输入压缩到-1--1之间。 R与SVM的不同之处在于二者损失函数不同，前者考虑全局的点，后者只考虑局部的边界线附近的点，其次在解决非线性的时候SVM会使用核函数，而LR不用，因为LR会考虑全部的点，如果用核函数计算量太大，SVM依赖数据表达的距离深度，因此需要做norm，而LR不用。SVM损失函数自带正则，而LR需要自己加正则。
4. RF、BT、GBDT、XGBOOST、BOOST、BAGGING、Stacking、LightGBM
   RF实际是bagging + 决策树，与传统决策树不同，他每次是采用留出法从属性集中随即选择K个属性的子集构建决策树，这样构建出来的多个树组成了随机森林，他的优点在于实现简单、计算开销小，兼具bagging的数据扰动和决策树的属性扰动，使得整体的性能得到了极大提升，泛化性能优异。
   BT：提升树，残差拟合+决策树,采用加法模型和前向分布算法，同时基函数采用决策树算法，拟合的是残差，即下一个树的输入是上一个树的残差。
   GBDT：不再以残差作为新的训练数据，而是使用损失函数的梯度作为新的训练数据的y值。重点在于特征选择，如何用于分类、多分类，与SVM等的比较。
5. PCA与LDA的区别
6. 其他机器学习模型诸如朴素贝叶斯、条件随机场、马尔科夫链等
7. OHEM、RefineDet、FocalLoss抑制正负样本失衡问题，RefineDet是SSD变种，在每层卷积后进行anchor的refine，然后transfer connection（主要与前一层进行连接）
8. RNN LSTM GRU 系列
9. Kmeans聚类为什么能收敛
10. BN原理，为什么能收敛，BN是怎么避免过拟合的，缺点，解决SyncBN，GN
   首先BN是批规范化，是通过mini-batch对相应的activition进行规范化操作，使得结果的均值为0，方差为1，而最后的“scale and shift”操作则是为了让因训练所需而“刻意”加入的，让BN能够有可能还原最初的输入，从而保证整个network的capacity。（有关capacity的解释：实际上BN可以看作是在原模型上加入的“新操作”，这个新操作很大可能会改变某层原来的输入。当然也可能不改变，不改变的时候就是“还原原来输入”。如此一来，既可以改变同时也可以保持原输入，那么模型的容纳能力（capacity）就提升了。BN主要用在非线性激活函数之前。那么BN为什么是work的呢，这就要和“Internal Covariate Shift”。文章的title除了BN这样一个关键词，还有一个便是“ICS”。大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如，transfer learning/domain adaptation等。而covariate shift就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。但是BN并不能解决，只是加了一层包装纸，其真正作用是解决梯度弥散和爆炸，在BN中，是通过将activation规范为均值和方差一致的手段使得原本会减小的activation的scale变大。可以说是一种更有效的local response normalization方法。
   BN核心是通过对系统参数搜索空间进行约束来增加系统鲁棒性，这种约束压缩了搜索空间，约束也改善了系统的结构合理性，这会带来一系列的性能改善，比如加速收敛，保证梯度，缓解过拟合等，缓解过拟合是由于在计算中使用了滑动平均、滑动方差，相当于加入了数据增广，因此可以缓解过拟合，但是不一定有用，加速收敛是因为约束了搜索空间使得参数选择更加容易。
   BN的缺点：在batch比较小的时候准确率会下降，在检测、行为识别等task上batch远远达不到分类那么大，且在action中会把loss拉的很低，影响收敛，都是把backbonedebn freeze掉。解决是引用SyncBN和GN，GN就是将[n,c,h,w]中将Channl分成很多组，对每一组做归一化，及先将feature的维度由[N, C, H, W]reshape为[N, G，C//G , H, W]，归一化的维度为[C//G , H, W]，而不是像BN那样在n维度上做归一化。传统角度来讲，在深度学习没有火起来之前，提取特征通常是使用SIFT，HOG和GIST特征，这些特征有一个共性，都具有按group表示的特性，每一个group由相同种类直方图的构建而成，这些特征通常是对在每个直方图（histogram）或每个方向（orientation）上进行组归一化（group-wise norm）而得到。而更高维的特征比如VLAD和Fisher Vectors(FV)也可以看作是group-wise feature，此处的group可以被认为是每个聚类（cluster）下的子向量sub-vector。从深度学习上来讲，完全可以认为卷积提取的特征是一种非结构化的特征或者向量，拿网络的第一层卷积为例，卷积层中的的卷积核filter1和此卷积核的其他经过transform过的版本filter2（transform可以是horizontal flipping等），在同一张图像上学习到的特征应该是具有相同的分布，那么，具有相同的特征可以被分到同一个group中，按照个人理解，每一层有很多的卷积核，这些核学习到的特征并不完全是独立的，某些特征具有相同的分布，因此可以被group，导致分组（group）的因素有很多，比如频率、形状、亮度和纹理等，HOG特征根据orientation分组，而对神经网络来讲，其提取特征的机制更加复杂，也更加难以描述，变得不那么直观。
   另在神经科学领域，一种被广泛接受的计算模型是对cell的响应做归一化，此现象存在于浅层视觉皮层和整个视觉系统。这样就可以解释为什么不用LN(c,h,w)来归一化或者IN（H,W）。
11. 在20W数据集上的过拟合问题、欠拟合、数据不均衡怎么解决、数据增广
   数据不均衡问题：数据增广、欠采样、过采样（其实针对类别少的样本进行过采样就是数据增广）、再缩放（参考周志华）、weight loss，但其实在物体检测task中这是一个比较难解决的问题，我们只能尽量去收集那些类别少的样本，过采样虽有一定的提升效果，但是效果不大，其实反而容易造成过拟合，
12. 自己的研究，利用RFCN的tracking、视频检测，IterBoxes，LSTM，Sigmoid。
13. IoN：在conv3-5上提取框的特征，然后在conv5后面接上RNN，在不同方向上探索，提取语义特征，然后将几个特征用L2正则连接起来做分类和回归。
    CCNET：对于一个proposal，pooling出不同大小的特征，然后各个尺寸分别做分类，用大尺寸的分数来筛选小分数的，决定对应的框是不是要被过滤掉。
    MR-RCNN：和MultiPATH差不多，不同的是用的原框，左半部分，右半部分，二倍作为ROI，然后单独接pooling和FC，然后concat做回归和分类。
    MultiPATH：也是不同size的框，每种size的框单独经过两层FC获取向量，然后做拼接作为最终的向量去做分类回归。
    DeepID：用可形变pooling获取特征，然后做200类分类，同时拿框获取图片丢入1000类的分类器去refine200类结果。
    HyperNet：不同卷积特征连在一起，然后提取特征走faster的路，连接在一起的卷积再经过一次卷积，然后提取特征。
    AC-CNN：对于一个proposal，选取不同size类似MR得到不同向量，用更大的size经过LSTM学习attention向量，然后将特征向量拼接。
    FPN、BPA、Adaptive Pooling
    SSD的各种变种，ESSD、DSOD:转置卷积连接，RON：后层连前层，然后单独做分类。RetinaNet、
14. Casecade RCNN：类似IterBox,不同的是每次迭代针对的是不同IOU的框，针对不同IOU学习分类器与回归器，然后做ensemble，boost的做法。
    IOUNet：首先是精确的Pooling，即采用积分的形式，另外就是加入IOU预测支路，框的置信度改为IOU预测得分，然后以这个置信度进行nms操作，同时
            还要考虑分类得分的问题，用IOU分数筛选出来的最大置信度的框，其分类score是所有抑制掉的框的score的最大值。
    IterBox：把上一次迭代的box拿过来继续分类迭代，类似MTCNN
    Integral Loss：Pooling之后学习不同的分类器，但是回归器只有一个。
15. DCN、bigbatch、DPooling、heavier header、Light Head。其中DCN与DPooling都是在原本的卷积或者ROI Pooling的基础上去加一层卷积或者全连接，
    学习卷积的偏移量，然后作用在原本的卷积核或者Pooling上，原本的矩形卷积核各个位置加上偏移量就变成了可以形变的卷积核。
16. NMS、SoftNMS、IOU guided NMS。softnms不是直接删除掉大于IOU阈值的框，而是根据其阈值调整score，s = s*(1-IOU)，IOU越大分数越小，然后进行分数筛选的时候就会被抑制掉。之所以这样是因为IOU的阈值不太容易确定，用NMS效果不好。
17. RFCN、Light-Head RCNN、MaskRCNN、YOLO、YOLOV2、YOLOV3、SSD、RetinaNet
18. 为什么one-stage比two-stage更容易正负样本失衡，因为RPN起到了一次过滤，正负样本失衡问题要轻一些。失衡的影响：在正负样本严重失衡的情况下进行细分类，softmax交叉熵损失无法抗衡这一问题，导致分类器训练失败。因此需要FocalLoss。
19. Relation Networks、GCN问题、行为识别+物体检测
20. 数据增强的方法：各种反转、Crop、
21. Tensorflow 优化器及其超参数的设置
22. Caffe源码 GPU加速体现，SyncedMem机制。
23. Float类型怎么存储，可以用cuda怎么优化。
24. 生成式模型：朴素贝叶斯、隐型马尔科夫模型。判别式模型：LR、SVM、KNN、感知器、决策树，生成模型都是根据联合概率通过贝叶斯公式转换为条件概率的，而判别模式不是，主要根据数据之间的差别进行判断。
25. weight Norm 与 BN对比：
   weight norm是对权重进行重写，而BN是对输入数据进行操作。前者比后者有如下优势：前者没有引入mini-batch，更适合于RNN等变长网络，前者通过标量g和向量v对权重重写，其中v是固定的，相比之下引入的噪声更少，计算开销比bn少，无需保存和计算滑动值。
26. 训练集、验证集、测试集：训练集学习样本数据集，通过匹配一些参数来建立一个分类器。建立一种分类的方式，主要是用来训练模型的。验证集：对学习出来的模型，调整分类器的参数，如在神经网络中选择隐藏单元数。验证集还用来确定网络结构或者控制模型复杂程度的参数。测试集主要是测试训练好的模型的分辨能力（识别率等）。
27. 数据预处理：
   数据归一化：逐样本减均值、归一化、特征标准化。归一化主要是想让样本各个维度一致，防止出现某些维度值特别大的情况。逐样本减均值则主要用于在那些具有稳定性的数据集中，也就是那些数据的每个维度间的统计性质是一样的。比如图片，像灰度图就是减去亮度的影响。而特征标准化是指对数据的每一维进行均值化和方差相等化，主要应用在机器学习中，如SVM等。
   白化：主要用于降低冗余性，使特征之间的相关性较低，所有特征具有相同的方差，一般会和PCA一起应用。
   PCA降维：
28. 检测模型效果对比：
29. 准确率、召回率计算、PR曲线、AUC曲线、混淆矩阵
30. 样本不均匀时如何解决：过采样、欠采样、再缩放、数据增强、weighted loss。如何判定过拟合：训练集的loss比测试集大很多，或者准确率之类的差很多。
31. Sigmoid函数可以增强0附近的梯度，放大信号，梯度计算方便，但对非敏感区域会饱和，造成梯度消失; Relu函数为单侧抑制，增强稀疏性，避免梯度消失; Softmax函数用于多类分类问题，将输出归一化，方便计算基于概率的loss，但是面对类间交叉的时候就难喽。
32. 滑动平均模型与BN mean、variance的更新，*****在检测和行为识别的时候是都要把bn freeze掉****，batch太小，这里其实是有有点问题的，只能说我们假设检测图片数据分布是和分类分布一致的，这也是为什么有的时候train from strach要好一点。
   滑动平均模型：分为简单滑动平均与加权滑动平均。滑动平均是将历史的不同时序的值都进行保存，有新值的时候加入求和计算当前值的平均值，这里tf是保存滑动平均值和对应的迭代次数，当有新值加入的时候就更新。简单滑动平均模型是所有值放在一起求平均，而加权平均是将离当前时刻比较远的值赋以小的权重，因为越远对当前的影响越小。BN貌似不是这样的，好像加入了惩罚项，算是加权吧，assign_moving_average(moving_variance, variance, BN_DECAY)。tf.nn.batch_norm 需要自己处理张量的均值和方差，而fused_batch_norm对输入的四维都进行了优化。
33. 快排最好情况的复杂度是o(nlogn),优化三路快排
34. mobilenet计算量推导
35. 网络为什么越复杂越不容易落到局部最优？复杂的定义是什么？网络初始化的时候方差对网络的影响？超参数有哪些？
    神经网络的复杂度，我们可以使用层数，神经元数目，或者连接权重数目作为度量。相对的，数据本身的复杂度，我们用带标签的数据的比例和不带标签的数据的比例来衡量。
    越复杂对数据的抽象做的越好，参数越多，在拟合的时候越不容易落在局部最优。
    超参数1. 学习率 η，2. 正则化参数 λ，3. 神经网络的层数 L，4. 每一个隐层中神经元的个数 j，5. 学习的回合数Epoch，6. 小批量数据 minibatch 的大小，7. 输出神经元的编码方式，8. 代价函数的选择，9. 权重初始化的方法，10. 神经元激活函数的种类，11.参加训练模型数据的规模 
36. PCA中特征值和特征向量的含义
37. 为什么一般网络后面要加两层fc：理论上两层fc可以拟合多数非线性函数了，多了的话会导致参数过多，效率差。
38. 为什么降采样用maxpooling而不是average pooling，二者的区别在哪里？
    虽然都是对特征进行整合，但是max pooling更像是对特征进行了选择，选择辨识度更高的特征，增加了非线性。特征提取的误差主要来自两个方面：（1）邻域大小受限造成的估计值方差增大；（2）卷积层参数误差造成估计均值的偏移。一般来说，average-pooling能减小第一种误差，更多的保留图像的背景信息，max-pooling能减小第二种误差，更多的保留纹理信息。average-pooling更强调对整体特征信息进行一层下采样，在减少参数维度的贡献上更大一点，更多的体现在信息的完整传递这个维度上，在一个很大很有代表性的模型中，比如说DenseNet中的模块之间的连接大多采用average-pooling，在减少维度的同时，更有利信息传递到下一个模块进行特征提取。所以在Inception、Resnet等深层网络中一般在最后进行average pooling，在前面会用到max pooling。
39. weight decay的作用在哪里？
    在损失函数中，weight decay是放在正则项（regularization）前面的一个系数，正则项一般指示模型的复杂度，所以weight decay的作用是调节模型复杂度对损失函数的影响，若weight decay很大，则复杂的模型损失函数的值也就大。
40. 基本网络原理，VGG参数计算，各自为什么work？
41. smooth L1Loss的选择原因是什么？公式是啥？L1 loss的导数是常数，可以防止数据中的离群点（噪声）产生的loss过大导致梯度爆炸2：原始L1导数在原点处不连续，于是在原点附近用二次函数代替。
42. 解释一下池化层的原理以及反传。
    主要是为了做特征整合，反传的时候average是将梯度等分进行反传，而max则是记录最大的位置，只更新最大位置的梯度，其他为0.
43. softmax与sigmoid的比较，softmax用于多分类的优点？计算简单，让大的更大，这样就会让错的更错，这样利于训练。
44. 隐层节点数确认：必须小于样本数，样本数一般是网络模型连接权数的2~10倍。一般来说增加层数和增加隐层节点数都可以
45. 剪枝是决策树算法抵抗过拟合的有效算法，在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因训练样本学得"太好"了，以致于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合.因此，可通过主动去掉一些分支来降低过拟合的风险.
    主要分为预剪枝和后剪枝，预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点;后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能，带来决策树泛化性能提升，则将该子树替换为叶结点.
    具体的做法就是使用留出法保留验证集用来验证是否有泛化性能的提升。
    后剪枝相对来说保留的分支更少，更不容易过拟合，但是开销比较大。
46. 从偏差--方差分解来看，boosting主要关注于降低偏差。
47. 牛顿法
48. RNN中梯度爆炸，梯度消失，怎么造成，怎么解决
49. 如果要构造一个卷积神经网络，你会考虑哪些方面，从原理方面去讲
50. AUC的意义，ROC的绘制方式，AUC的优势（不平衡数据集的情况）：真正例率与假正例率，AUC是在两个学习器的ROC有交叉时不容易区分优劣，因此采用曲线下的面积进行区分。
51. 不平衡数据的解决方式，数据分布改变了怎么办
52. 注意力机制是怎么做的，注意力公式
53. CNN为什么比DNN在图像识别上更好:因为DNN输入是向量，考虑不到平面结构信息
54. 交叉熵公式，为什么使用呢
55. relu的缺点：参考mobilenet v2的讨论
56. 具体解释Adam，二阶矩是什么，为什么用它：
57. 正则化与bias，variance的关系：高偏差对应着欠拟合，应减少正则化系数，高方差对应着过拟合，应增大正则化系数
58. 凸函数是什么，有什么良好的性质，极值是啥？








