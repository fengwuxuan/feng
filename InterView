1. 介绍项目，推导MV1与MV2，tracking模块，检测模块的改进，在此基础上看对方是否了解行为识别，决定石油项目怎么讲。
2. L0、L1 与 L2的区别，为什么L1 会造成稀疏权重，L2造成趋近于0的权重,加了L1、L2为什么还会收敛
   答：L0是非零元素的个数，实现特征的自动选择，去除无用特征。稀疏化可以去掉这些无用特征，将特征对应的权重置为零。L1是各元素的绝对值之和，L1范数也可以实现稀疏，
   通过将无用特征对应的参数W置为零实现。L0和L1都可以实现稀疏化，不过一般选用L1而不用L0，原因包括：1）L0范数很难优化求解（NP难）；2）L1是L0的最优凸近似，比L0
   更容易优化求解。L2是各元素平方和再开方，L2避免过拟合的原理是：让L2范数的规则项||W||2 尽可能小，可以使得W每个元素都很小，接近于零，但是与L1不同的是，不会等
   于0；这样得到的模型抗干扰能力强，参数很小时，即使样本数据x发生很大的变化，模型预测值y的变化也会很有限。
3. LR、SVM推导、核函数
4. RF、DT、GBDT、XGBOOST、BOOST、BAGGING
5. PCA与LDA的区别
6. 其他机器学习模型诸如朴素贝叶斯、条件随机场、马尔科夫链等
7. OHEM、RefineDet、FocalLoss抑制正负样本失衡问题，RefineDet是SSD变种，在每层卷积后进行anchor的refine，然后transfer connection（主要与前一层进行连接）
8. RNN LSTM GRU 系列
9. 聚类为什么能收敛
10. BN为什么能收敛，BN是怎么避免过拟合的
11. 在20W数据集上的过拟合问题、欠拟合、数据不均衡怎么解决、数据增广
12. 自己的研究，利用RFCN的tracking、视频检测，IterBoxes，LSTM，Sigmoid。
13. IoN：在conv3-5上提取框的特征，然后在conv5后面接上RNN，在不同方向上探索，提取语义特征，然后将几个特征用L2正则连接起来做分类和回归。
    CCNET：对于一个proposal，pooling出不同大小的特征，然后各个尺寸分别做分类，用大尺寸的分数来筛选小分数的，决定对应的框是不是要被过滤掉。
    MR-RCNN：和MultiPATH差不多，不同的是用的原框，左半部分，右半部分，二倍作为ROI，然后单独接pooling和FC，然后concat做回归和分类。
    MultiPATH：也是不同size的框，每种size的框单独经过两层FC获取向量，然后做拼接作为最终的向量去做分类回归。
    DeepID：用可形变pooling获取特征，然后做200类分类，同时拿框获取图片丢入1000类的分类器去refine200类结果。
    HyperNet：不同卷积特征连在一起，然后提取特征走faster的路，连接在一起的卷积再经过一次卷积，然后提取特征。
    AC-CNN：对于一个proposal，选取不同size类似MR得到不同向量，用更大的size经过LSTM学习attention向量，然后将特征向量拼接。
    FPN、BPA、Adaptive Pooling
    SSD的各种变种，ESSD、DSOD:转置卷积连接，RON：后层连前层，然后单独做分类。RetinaNet、
14. Casecade RCNN：类似IterBox,不同的是每次迭代针对的是不同IOU的框，针对不同IOU学习分类器与回归器，然后做ensemble，boost的做法。
    IOUNet：首先是精确的Pooling，即采用积分的形式，另外就是加入IOU预测支路，框的置信度改为IOU预测得分，然后以这个置信度进行nms操作，同时
            还要考虑分类得分的问题，用IOU分数筛选出来的最大置信度的框，其分类score是所有抑制掉的框的score的最大值。
    IterBox：把上一次迭代的box拿过来继续分类迭代，类似MTCNN
    Integral Loss：Pooling之后学习不同的分类器，但是回归器只有一个。
15. DCN、bigbatch、DPooling、heavier header、Light Head。其中DCN与DPooling都是在原本的卷积或者ROI Pooling的基础上去加一层卷积或者全连接，
    学习卷积的偏移量，然后作用在原本的卷积核或者Pooling上，原本的矩形卷积核各个位置加上偏移量就变成了可以形变的卷积核。
16. NMS、SoftNMS、IOU guided NMS。softnms不是直接删除掉大于IOU阈值的框，而是根据其阈值调整score，s = s*(1-IOU)
，IOU越大分数越小，然后进行分数筛选的时候就会被抑制掉。
17. RFCN、Light-Head RCNN、MaskRCNN、YOLO、YOLOV2、YOLOV3、SSD、RetinaNet
18. 为什么one-stage比two-stage更容易正负样本失衡，因为RPN起到了一次过滤，正负样本失衡问题要轻一些。失衡的影响：在正负样本严重失衡的情况下进行细分类，
softmax交叉熵损失无法抗衡这一问题，导致分类器训练失败。因此需要FocalLoss。
19. Relation Networks、GCN问题、行为识别+物体检测
20. 数据增强的方法
21. 什么情况下可以用核函数
22. Tensorflow 优化器及其超参数的设置
23. Caffe源码 GPU加速体现，SyncedMem机制
24. Float类型怎么存储，可以用cuda怎么优化
