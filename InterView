1. 介绍项目，推导MV1与MV2，tracking模块，检测模块的改进，在此基础上看对方是否了解行为识别，决定石油项目怎么讲。
2. L0、L1 与 L2的区别，为什么L1 会造成稀疏权重，L2造成趋近于0的权重,加了L1、L2为什么还会收敛
   答：L0是非零元素的个数，实现特征的自动选择，去除无用特征。稀疏化可以去掉这些无用特征，将特征对应的权重置为零。L1是各元素的绝对值之和，L1范数也可以实现稀疏，
   通过将无用特征对应的参数W置为零实现。L0和L1都可以实现稀疏化，不过一般选用L1而不用L0，原因包括：1）L0范数很难优化求解（NP难）；2）L1是L0的最优凸近似，比L0
   更容易优化求解。L2是各元素平方和再开方，L2避免过拟合的原理是：让L2范数的规则项||W||2 尽可能小，可以使得W每个元素都很小，接近于零，但是与L1不同的是，不会等
   于0；这样得到的模型抗干扰能力强，参数很小时，即使样本数据x发生很大的变化，模型预测值y的变化也会很有限。L1造成稀疏可以用参数更新来解释，在迭代几次之和
   容易求得0值，L2则是容易靠近到0，而不是直接减为0.
   加上他们的好处在于参数稀疏化，可以简化模型，避免过拟合，因为真正可用的参数并不多。使得参数值变小，参数越小模型越简单。
3. LR、SVM推导、核函数，什么时候用核函数，SVM输入怎么选择，LR和SVM怎么选择
   当特征在低维不可分时，需要利用f(x)将x映射到高维空间，但是会出现内积f(i)f(j)的计算，其计算维度会非常的大，因此利用核函数进行计算。常见核函数的展开
   及其参数调整，有两个重要的参数即C、Gamma，前者为惩罚项，越大容易过拟合，越小容易欠拟合，后者越大支持向量越少，越小支持向量越大。至于特征选择不是很熟悉，
   了解一下鸟窝的处理，SVM好像要把输入压缩到-1--1之间。LR与SVM的不同之处在于二者损失函数不同，前者考虑全局的点，后者只考虑局部的边界线附近的点，其次在
   解决非线性的时候SVM会使用核函数，而LR不用，因为LR会考虑全部的点，如果用核函数计算量太大，SVM依赖数据表达的距离深度，因此需要做norm，而LR不用。SVM损失函数
   自带正则，而LR需要自己加正则。
4. RF、DT、GBDT、XGBOOST、BOOST、BAGGING
5. PCA与LDA的区别
6. 其他机器学习模型诸如朴素贝叶斯、条件随机场、马尔科夫链等
7. OHEM、RefineDet、FocalLoss抑制正负样本失衡问题，RefineDet是SSD变种，在每层卷积后进行anchor的refine，然后transfer connection（主要与前一层进行连接）
8. RNN LSTM GRU 系列
9. Kmeans聚类为什么能收敛
10. BN原理，为什么能收敛，BN是怎么避免过拟合的，缺点，解决SyncBN，GN
11. 在20W数据集上的过拟合问题、欠拟合、数据不均衡怎么解决、数据增广
12. 自己的研究，利用RFCN的tracking、视频检测，IterBoxes，LSTM，Sigmoid。
13. IoN：在conv3-5上提取框的特征，然后在conv5后面接上RNN，在不同方向上探索，提取语义特征，然后将几个特征用L2正则连接起来做分类和回归。
    CCNET：对于一个proposal，pooling出不同大小的特征，然后各个尺寸分别做分类，用大尺寸的分数来筛选小分数的，决定对应的框是不是要被过滤掉。
    MR-RCNN：和MultiPATH差不多，不同的是用的原框，左半部分，右半部分，二倍作为ROI，然后单独接pooling和FC，然后concat做回归和分类。
    MultiPATH：也是不同size的框，每种size的框单独经过两层FC获取向量，然后做拼接作为最终的向量去做分类回归。
    DeepID：用可形变pooling获取特征，然后做200类分类，同时拿框获取图片丢入1000类的分类器去refine200类结果。
    HyperNet：不同卷积特征连在一起，然后提取特征走faster的路，连接在一起的卷积再经过一次卷积，然后提取特征。
    AC-CNN：对于一个proposal，选取不同size类似MR得到不同向量，用更大的size经过LSTM学习attention向量，然后将特征向量拼接。
    FPN、BPA、Adaptive Pooling
    SSD的各种变种，ESSD、DSOD:转置卷积连接，RON：后层连前层，然后单独做分类。RetinaNet、
14. Casecade RCNN：类似IterBox,不同的是每次迭代针对的是不同IOU的框，针对不同IOU学习分类器与回归器，然后做ensemble，boost的做法。
    IOUNet：首先是精确的Pooling，即采用积分的形式，另外就是加入IOU预测支路，框的置信度改为IOU预测得分，然后以这个置信度进行nms操作，同时
            还要考虑分类得分的问题，用IOU分数筛选出来的最大置信度的框，其分类score是所有抑制掉的框的score的最大值。
    IterBox：把上一次迭代的box拿过来继续分类迭代，类似MTCNN
    Integral Loss：Pooling之后学习不同的分类器，但是回归器只有一个。
15. DCN、bigbatch、DPooling、heavier header、Light Head。其中DCN与DPooling都是在原本的卷积或者ROI Pooling的基础上去加一层卷积或者全连接，
    学习卷积的偏移量，然后作用在原本的卷积核或者Pooling上，原本的矩形卷积核各个位置加上偏移量就变成了可以形变的卷积核。
16. NMS、SoftNMS、IOU guided NMS。softnms不是直接删除掉大于IOU阈值的框，而是根据其阈值调整score，s = s*(1-IOU)
，IOU越大分数越小，然后进行分数筛选的时候就会被抑制掉。
17. RFCN、Light-Head RCNN、MaskRCNN、YOLO、YOLOV2、YOLOV3、SSD、RetinaNet
18. 为什么one-stage比two-stage更容易正负样本失衡，因为RPN起到了一次过滤，正负样本失衡问题要轻一些。失衡的影响：在正负样本严重失衡的情况下进行细分类，
softmax交叉熵损失无法抗衡这一问题，导致分类器训练失败。因此需要FocalLoss。
19. Relation Networks、GCN问题、行为识别+物体检测
20. 数据增强的方法
21. Tensorflow 优化器及其超参数的设置
22. Caffe源码 GPU加速体现，SyncedMem机制。
23. Float类型怎么存储，可以用cuda怎么优化。
24. 生成式模型：朴素贝叶斯、隐型马尔科夫模型。判别式模型：LR、SVM、KNN、感知器、决策树，生成模型都是根据联合概率通过贝叶斯公式转换为条件概率的，而判别模式不是，主要根据数据
    之间的差别进行判断。
